[
    {
        "ip": "192.0.0.1",
        "port": "1027",
        "identity": "P",
        "NPU_mem_size": 1024,
        "metrics_str": "# HELP request_received_total Number of requests received so far.\n# TYPE request_received_total counter\nrequest_received_total{model_name=\"llama_65b\"} 100\n# HELP request_success_total Count of successfully processed requests.\n# TYPE request_success_total counter\nrequest_success_total{model_name=\"llama_65b\"} 100\n# HELP request_failed_total Number of responses failed so far\n# TYPE request_failed_total counter\nrequest_failed_total{model_name=\"llama_65b\"} 0\n# HELP prompt_tokens_total Number of prefill tokens processed.\n# TYPE prompt_tokens_total counter\nprompt_tokens_total{model_name=\"llama_65b\"} 78800\n# HELP generation_tokens_total Number of generation tokens processed.\n# TYPE generation_tokens_total counter\ngeneration_tokens_total{model_name=\"llama_65b\"} 80000\n# HELP num_preemptions_total Cumulative number of preemption from the engine.\n# TYPE num_preemptions_total counter\nnum_preemptions_total{model_name=\"llama_65b\"} 1274\n# HELP num_requests_running Number of requests currently running on GPU.\n# TYPE num_requests_running gauge\nnum_requests_running{model_name=\"llama_65b\"} 0\n# HELP num_requests_waiting Number of requests waiting to be processed.\n# TYPE num_requests_waiting gauge\nnum_requests_waiting{model_name=\"llama_65b\"} 0\n# HELP num_requests_swapped Number of requests swapped to CPU.\n# TYPE num_requests_swapped gauge\nnum_requests_swapped{model_name=\"llama_65b\"} 0\n# HELP avg_prompt_throughput_toks_per_s Average prefill throughput in tokens/s.\n# TYPE avg_prompt_throughput_toks_per_s gauge\navg_prompt_throughput_toks_per_s{model_name=\"llama_65b\"} 0\n# HELP avg_generation_throughput_toks_per_s Average generation throughput in tokens/s.\n# TYPE avg_generation_throughput_toks_per_s gauge\navg_generation_throughput_toks_per_s{model_name=\"llama_65b\"} 0\n# HELP failed_request_perc Requests failure rate. 1 means 100 percent usage.\n# TYPE failed_request_perc gauge\nfailed_request_perc{model_name=\"llama_65b\"} 0\n# HELP npu_cache_usage_perc NPU KV-cache usage. 1 means 100 percent usage.\n# TYPE npu_cache_usage_perc gauge\nnpu_cache_usage_perc{model_name=\"llama_65b\"} 0.1\n# HELP cpu_cache_usage_perc CPU KV-cache usage. 1 means 100 percent usage.\n# TYPE cpu_cache_usage_perc gauge\ncpu_cache_usage_perc{model_name=\"llama_65b\"} 0.2\n# HELP npu_prefix_cache_hit_rate NPU prefix cache block hit rate..\n# TYPE npu_prefix_cache_hit_rate gauge\nnpu_prefix_cache_hit_rate{model_name=\"llama_65b\"} 0.1\n# HELP time_to_first_token_seconds Histogram of time to first token in seconds.\n# TYPE time_to_first_token_seconds histogram\ntime_to_first_token_seconds_count{model_name=\"llama_65b\"} 400\ntime_to_first_token_seconds_sum{model_name=\"llama_65b\"} 5008.75\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.001\"} 0\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.005\"} 0\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.01\"} 0\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.02\"} 0\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.04\"} 0\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.06\"} 4\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.08\"} 4\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.1\"} 4\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.25\"} 36\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.5\"} 64\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.75\"} 64\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"1\"} 64\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"2.5\"} 64\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"5\"} 128\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"7.5\"} 148\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"10\"} 148\ntime_to_first_token_seconds_bucket{model_name=\"llama_65b\",le=\"+Inf\"} 400\n# HELP time_per_output_token_seconds Histogram of time per output token in seconds.\n# TYPE time_per_output_token_seconds histogram\ntime_per_output_token_seconds_count{model_name=\"llama_65b\"} 79600\ntime_per_output_token_seconds_sum{model_name=\"llama_65b\"} 7129.51\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.01\"} 0\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.025\"} 70390\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.05\"} 72902\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.075\"} 75828\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.1\"} 76654\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.15\"} 77528\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.2\"} 77968\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.3\"} 78444\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.4\"} 78576\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.5\"} 78746\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"0.75\"} 79012\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"1\"} 79104\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"2.5\"} 79236\ntime_per_output_token_seconds_bucket{model_name=\"llama_65b\",le=\"+Inf\"} 79600\n# HELP e2e_request_latency_seconds Histogram of end to end request latency in seconds.\n# TYPE e2e_request_latency_seconds histogram\ne2e_request_latency_seconds_count{model_name=\"llama_65b\"} 400\ne2e_request_latency_seconds_sum{model_name=\"llama_65b\"} 12177.4\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"1\"} 0\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"2.5\"} 0\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"5\"} 32\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"10\"} 42\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"15\"} 84\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"20\"} 118\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"30\"} 178\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"40\"} 270\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"50\"} 350\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"60\"} 400\ne2e_request_latency_seconds_bucket{model_name=\"llama_65b\",le=\"+Inf\"} 400\n# HELP request_prompt_tokens Number of prefill tokens processed.\n# TYPE request_prompt_tokens histogram\nrequest_prompt_tokens_count{model_name=\"llama_65b\"} 400\nrequest_prompt_tokens_sum{model_name=\"llama_65b\"} 78800\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"10\"} 0\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"50\"} 0\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"100\"} 0\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"200\"} 400\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"500\"} 400\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"1000\"} 400\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"2000\"} 400\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"5000\"} 400\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"10000\"} 400\nrequest_prompt_tokens_bucket{model_name=\"llama_65b\",le=\"+Inf\"} 400\n# HELP request_generation_tokens Number of generation tokens processed.\n# TYPE request_generation_tokens histogram\nrequest_generation_tokens_count{model_name=\"llama_65b\"} 400\nrequest_generation_tokens_sum{model_name=\"llama_65b\"} 80000\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"10\"} 0\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"50\"} 0\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"100\"} 0\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"200\"} 400\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"500\"} 400\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"1000\"} 400\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"2000\"} 400\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"5000\"} 400\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"10000\"} 400\nrequest_generation_tokens_bucket{model_name=\"llama_65b\",le=\"+Inf\"} 400\n"
    }
]