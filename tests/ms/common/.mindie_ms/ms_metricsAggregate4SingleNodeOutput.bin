# HELP request_received_total Number of requests received so far.
# TYPE request_received_total counter
request_received_total{model_name="llama_65b"} 0
# HELP request_success_total Count of successfully processed requests.
# TYPE request_success_total counter
request_success_total{model_name="llama_65b"} 0
# HELP request_failed_total Number of responses failed so far
# TYPE request_failed_total counter
request_failed_total{model_name="llama_65b"} 0
# HELP prompt_tokens_total Number of prefill tokens processed.
# TYPE prompt_tokens_total counter
prompt_tokens_total{model_name="llama_65b"} 195200
# HELP generation_tokens_total Number of generation tokens processed.
# TYPE generation_tokens_total counter
generation_tokens_total{model_name="llama_65b"} 200000
# HELP num_preemptions_total Cumulative number of preemption from the engine.
# TYPE num_preemptions_total counter
num_preemptions_total{model_name="llama_65b"} 14425
# HELP num_requests_running Number of requests currently running on GPU.
# TYPE num_requests_running gauge
num_requests_running{model_name="llama_65b"} 0
# HELP num_requests_waiting Number of requests waiting to be processed.
# TYPE num_requests_waiting gauge
num_requests_waiting{model_name="llama_65b"} 0
# HELP num_requests_swapped Number of requests swapped to CPU.
# TYPE num_requests_swapped gauge
num_requests_swapped{model_name="llama_65b"} 0
# HELP avg_prompt_throughput_toks_per_s Average prefill throughput in tokens/s.
# TYPE avg_prompt_throughput_toks_per_s gauge
avg_prompt_throughput_toks_per_s{model_name="llama_65b"} 0
# HELP avg_generation_throughput_toks_per_s Average generation throughput in tokens/s.
# TYPE avg_generation_throughput_toks_per_s gauge
avg_generation_throughput_toks_per_s{model_name="llama_65b"} 0
# HELP failed_request_perc Requests failure rate. 1 means 100 percent usage.
# TYPE failed_request_perc gauge
failed_request_perc{model_name="llama_65b"} 0
# HELP npu_cache_usage_perc NPU KV-cache usage. 1 means 100 percent usage.
# TYPE npu_cache_usage_perc gauge
npu_cache_usage_perc{model_name="llama_65b"} 0.3
# HELP cpu_cache_usage_perc CPU KV-cache usage. 1 means 100 percent usage.
# TYPE cpu_cache_usage_perc gauge
cpu_cache_usage_perc{model_name="llama_65b"} 0.4400000000000001
# HELP npu_prefix_cache_hit_rate NPU prefix cache block hit rate..
# TYPE npu_prefix_cache_hit_rate gauge
npu_prefix_cache_hit_rate{model_name="llama_65b"} 0.25
# HELP time_to_first_token_seconds Histogram of time to first token in seconds.
# TYPE time_to_first_token_seconds histogram
time_to_first_token_seconds_count{model_name="llama_65b"} 5846
time_to_first_token_seconds_sum{model_name="llama_65b"} 29497.504006
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.001"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.005"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.01"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.02"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.04"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.06"} 28
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.08"} 116
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.1"} 216
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.25"} 584
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.5"} 640
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.75"} 680
time_to_first_token_seconds_bucket{model_name="llama_65b",le="1"} 770
time_to_first_token_seconds_bucket{model_name="llama_65b",le="2.5"} 1384
time_to_first_token_seconds_bucket{model_name="llama_65b",le="5"} 2552
time_to_first_token_seconds_bucket{model_name="llama_65b",le="7.5"} 5342
time_to_first_token_seconds_bucket{model_name="llama_65b",le="10"} 5342
time_to_first_token_seconds_bucket{model_name="llama_65b",le="+Inf"} 5846
# HELP time_per_output_token_seconds Histogram of time per output token in seconds.
# TYPE time_per_output_token_seconds histogram
time_per_output_token_seconds_count{model_name="llama_65b"} 330800
time_per_output_token_seconds_sum{model_name="llama_65b"} 23150.73402565202
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.01"} 0
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.025"} 140780
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.05"} 145810
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.075"} 151680
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.1"} 233874
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.15"} 321346
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.2"} 322614
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.3"} 323566
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.4"} 324230
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.5"} 327770
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.75"} 329504
time_per_output_token_seconds_bucket{model_name="llama_65b",le="1"} 329808
time_per_output_token_seconds_bucket{model_name="llama_65b",le="2.5"} 330072
time_per_output_token_seconds_bucket{model_name="llama_65b",le="+Inf"} 330800
# HELP e2e_request_latency_seconds Histogram of end to end request latency in seconds.
# TYPE e2e_request_latency_seconds histogram
e2e_request_latency_seconds_count{model_name="llama_65b"} 5334
e2e_request_latency_seconds_sum{model_name="llama_65b"} 49723.8639961958
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="1"} 54
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="2.5"} 536
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="5"} 1488
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="10"} 4618
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="15"} 4702
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="20"} 4770
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="30"} 4890
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="40"} 5074
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="50"} 5234
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="60"} 5334
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="+Inf"} 5334
# HELP request_prompt_tokens Number of prefill tokens processed.
# TYPE request_prompt_tokens histogram
request_prompt_tokens_count{model_name="llama_65b"} 7176
request_prompt_tokens_sum{model_name="llama_65b"} 176728
request_prompt_tokens_bucket{model_name="llama_65b",le="10"} 6376
request_prompt_tokens_bucket{model_name="llama_65b",le="50"} 6376
request_prompt_tokens_bucket{model_name="llama_65b",le="100"} 6376
request_prompt_tokens_bucket{model_name="llama_65b",le="200"} 7176
request_prompt_tokens_bucket{model_name="llama_65b",le="500"} 7176
request_prompt_tokens_bucket{model_name="llama_65b",le="1000"} 7176
request_prompt_tokens_bucket{model_name="llama_65b",le="2000"} 7176
request_prompt_tokens_bucket{model_name="llama_65b",le="5000"} 7176
request_prompt_tokens_bucket{model_name="llama_65b",le="10000"} 7176
request_prompt_tokens_bucket{model_name="llama_65b",le="+Inf"} 7176
# HELP request_generation_tokens Number of generation tokens processed.
# TYPE request_generation_tokens histogram
request_generation_tokens_count{model_name="llama_65b"} 5334
request_generation_tokens_sum{model_name="llama_65b"} 328850
request_generation_tokens_bucket{model_name="llama_65b",le="10"} 0
request_generation_tokens_bucket{model_name="llama_65b",le="50"} 4534
request_generation_tokens_bucket{model_name="llama_65b",le="100"} 4534
request_generation_tokens_bucket{model_name="llama_65b",le="200"} 5334
request_generation_tokens_bucket{model_name="llama_65b",le="500"} 5334
request_generation_tokens_bucket{model_name="llama_65b",le="1000"} 5334
request_generation_tokens_bucket{model_name="llama_65b",le="2000"} 5334
request_generation_tokens_bucket{model_name="llama_65b",le="5000"} 5334
request_generation_tokens_bucket{model_name="llama_65b",le="10000"} 5334
request_generation_tokens_bucket{model_name="llama_65b",le="+Inf"} 5334
