# HELP request_received_total Number of requests received so far.
# TYPE request_received_total counter
request_received_total{model_name="llama_65b"} 0
# HELP request_success_total Count of successfully processed requests.
# TYPE request_success_total counter
request_success_total{model_name="llama_65b"} 0
# HELP request_failed_total Number of responses failed so far
# TYPE request_failed_total counter
request_failed_total{model_name="llama_65b"} 0
# HELP prompt_tokens_total Number of prefill tokens processed.
# TYPE prompt_tokens_total counter
prompt_tokens_total{model_name="llama_65b"} 28800
# HELP generation_tokens_total Number of generation tokens processed.
# TYPE generation_tokens_total counter
generation_tokens_total{model_name="llama_65b"} 60000
# HELP num_preemptions_total Cumulative number of preemption from the engine.
# TYPE num_preemptions_total counter
num_preemptions_total{model_name="llama_65b"} 6512
# HELP num_requests_running Number of requests currently running on GPU.
# TYPE num_requests_running gauge
num_requests_running{model_name="llama_65b"} 0
# HELP num_requests_waiting Number of requests waiting to be processed.
# TYPE num_requests_waiting gauge
num_requests_waiting{model_name="llama_65b"} 0
# HELP num_requests_swapped Number of requests swapped to CPU.
# TYPE num_requests_swapped gauge
num_requests_swapped{model_name="llama_65b"} 0
# HELP avg_prompt_throughput_toks_per_s Average prefill throughput in tokens/s.
# TYPE avg_prompt_throughput_toks_per_s gauge
avg_prompt_throughput_toks_per_s{model_name="llama_65b"} 0
# HELP avg_generation_throughput_toks_per_s Average generation throughput in tokens/s.
# TYPE avg_generation_throughput_toks_per_s gauge
avg_generation_throughput_toks_per_s{model_name="llama_65b"} 0
# HELP failed_request_perc Requests failure rate. 1 means 100 percent usage.
# TYPE failed_request_perc gauge
failed_request_perc{model_name="llama_65b"} 0
# HELP npu_cache_usage_perc NPU KV-cache usage. 1 means 100 percent usage.
# TYPE npu_cache_usage_perc gauge
npu_cache_usage_perc{model_name="llama_65b"} 0.2
# HELP cpu_cache_usage_perc CPU KV-cache usage. 1 means 100 percent usage.
# TYPE cpu_cache_usage_perc gauge
cpu_cache_usage_perc{model_name="llama_65b"} 0.3
# HELP npu_prefix_cache_hit_rate NPU prefix cache block hit rate..
# TYPE npu_prefix_cache_hit_rate gauge
npu_prefix_cache_hit_rate{model_name="llama_65b"} 0.2
# HELP time_to_first_token_seconds Histogram of time to first token in seconds.
# TYPE time_to_first_token_seconds histogram
time_to_first_token_seconds_count{model_name="llama_65b"} 2523
time_to_first_token_seconds_sum{model_name="llama_65b"} 9740.002003
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.001"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.005"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.01"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.02"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.04"} 0
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.06"} 10
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.08"} 54
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.1"} 104
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.25"} 256
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.5"} 256
time_to_first_token_seconds_bucket{model_name="llama_65b",le="0.75"} 276
time_to_first_token_seconds_bucket{model_name="llama_65b",le="1"} 321
time_to_first_token_seconds_bucket{model_name="llama_65b",le="2.5"} 628
time_to_first_token_seconds_bucket{model_name="llama_65b",le="5"} 1148
time_to_first_token_seconds_bucket{model_name="llama_65b",le="7.5"} 2523
time_to_first_token_seconds_bucket{model_name="llama_65b",le="10"} 2523
time_to_first_token_seconds_bucket{model_name="llama_65b",le="+Inf"} 2523
# HELP time_per_output_token_seconds Histogram of time per output token in seconds.
# TYPE time_per_output_token_seconds histogram
time_per_output_token_seconds_count{model_name="llama_65b"} 85800
time_per_output_token_seconds_sum{model_name="llama_65b"} 4445.85701282601
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.01"} 0
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.025"} 0
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.05"} 3
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.075"} 12
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.1"} 40283
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.15"} 83145
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.2"} 83339
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.3"} 83339
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.4"} 83539
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.5"} 85139
time_per_output_token_seconds_bucket{model_name="llama_65b",le="0.75"} 85740
time_per_output_token_seconds_bucket{model_name="llama_65b",le="1"} 85800
time_per_output_token_seconds_bucket{model_name="llama_65b",le="2.5"} 85800
time_per_output_token_seconds_bucket{model_name="llama_65b",le="+Inf"} 85800
# HELP e2e_request_latency_seconds Histogram of end to end request latency in seconds.
# TYPE e2e_request_latency_seconds histogram
e2e_request_latency_seconds_count{model_name="llama_65b"} 2267
e2e_request_latency_seconds_sum{model_name="llama_65b"} 12684.5319980979
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="1"} 27
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="2.5"} 268
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="5"} 712
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="10"} 2267
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="15"} 2267
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="20"} 2267
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="30"} 2267
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="40"} 2267
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="50"} 2267
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="60"} 2267
e2e_request_latency_seconds_bucket{model_name="llama_65b",le="+Inf"} 2267
# HELP request_prompt_tokens Number of prefill tokens processed.
# TYPE request_prompt_tokens histogram
request_prompt_tokens_count{model_name="llama_65b"} 3188
request_prompt_tokens_sum{model_name="llama_65b"} 9564
request_prompt_tokens_bucket{model_name="llama_65b",le="10"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="50"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="100"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="200"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="500"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="1000"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="2000"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="5000"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="10000"} 3188
request_prompt_tokens_bucket{model_name="llama_65b",le="+Inf"} 3188
# HELP request_generation_tokens Number of generation tokens processed.
# TYPE request_generation_tokens histogram
request_generation_tokens_count{model_name="llama_65b"} 2267
request_generation_tokens_sum{model_name="llama_65b"} 84425
request_generation_tokens_bucket{model_name="llama_65b",le="10"} 0
request_generation_tokens_bucket{model_name="llama_65b",le="50"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="100"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="200"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="500"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="1000"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="2000"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="5000"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="10000"} 2267
request_generation_tokens_bucket{model_name="llama_65b",le="+Inf"} 2267
