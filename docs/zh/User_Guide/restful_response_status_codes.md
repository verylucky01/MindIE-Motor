# RESTful响应状态码

|错误码|错误说明|返回结果|错误信息列表|
|--|--|--|--|
|200|success|OK|-|
|400|Bad Request|{<br>"type": "Bad Request",<br>"error": error message，详见错误信息列表<br>}|<li>"Force P release link failed."<li>"DMI inference requests can be sent only to the prefill node."<li>"DMI request must have req-type, req-id, d-target headers."<li>"The length of req-id cannot exceed 1024."<li>"req-type must be prefill but got {reqType}."<li>"d-target must be an IPv4 address."<li>"d-target should not be itself."<li>"is-recompute is an optional parameter. When it is set, it must be true or false."<li>"Non DMI does not support dresult request."<li>"Only the Decode node supports dresult but you send to " + {pdRole}."|
|404|NotFoundError|{<br>"type": "NotFoundError",<br>"error": error message，详见错误信息列表<br>}|<li>"Stop request id {stopReqId} not found."<li>"Max wait time is {CV_WAIT_TIME}, input is invalid or too long."<li>"Wait time should be in range of [1, {CV_WAIT_TIME}], input is not valid."<li>"Model {modelName} not found."|
|422|Input Validation Error|{<br>"type": "Input Validation Error",<br>"error": error message，详见错误信息列表<br>}|<li>"Failed to parse context to infer param."<li>"Failed to parse request context to json"<li>"Request contains not model or model null."<li>"Request param model must be string."<li>"Request param model must be not empty."<li>"Model not found."<li>"Tools not array."<li>"Tool param invalid, tool type not exist."<li>"Tool param invalid tool type."<li>"Tool param invalid tool string type."<li>"Tool param not function type."<li>"Tool param invalid, tool function not exist."<li>"Tool param invalid function object."<li>"Function object invalid, function name not exist."<li>"Function object invalid name type."<li>"The name of function must be a-z, A-Z, 0-9, underscores and dashs within max length of 64"<li>"tool_choice string not in [none, auto, required]."<li>"tool_choice not string or object."<li>"Messages MUST be a string for recompute."<li>"Request param contains not messages or messages null."<li>"Request param messages not arr or arr is empty."<li>"Request param contains not role or role null."<li>"Request param contains role must be system, assistant, user or tool."<li>"Request param contains not tool_call_id or tool_call_id null while role is tool."<li>"Request param tool_call_id len should not be 0 while role is tool."<li>"The type of tool_call_id is abnormal."<li>"Tool call param contains no id."<li>"The type of the tool. Currently, only function is supported."<li>"Tool call param contains no function that the model called."<li>"Tool call param contains no function name."<li>"Tool call param function name expected to be a string."<li>"Tool call param contains no function arguments."<li>"Tool call param function arguments expected to be a string."<li>"Request param contains not content or content null."<li>"Request param content len should not be 0."<li>"Request param contains no tool_call_id."<li>"Tools is empty"<li>"Messages parsed not is a array"<li>"Messages len not in (0, {MAX_INPUTS_NUM}], but the length of inputs is {utf16.length()}"<li>"Request param contains not messages or messages null"<li>"Request param messages not arr or arr is empty"<li>"Request param contains not role or role null"<li>"Request param contains role must be system, assistant, user or tool"<li>"Request param contains not content or content null"<li>"Request param content len should not be 0"<li>"Inputs arr is empty."<li>"Request param image_url null."<li>"Request param audio_url null."<li>"Request param video_url null."<li>"The number of multimodal url. should be no more than {MAX_MULTIMODAL_URL_NUM}."<li>"The type of inputs is abnormal."<li>"Failed to get token from input: {msg}."<li>"Unknown error."<li>"Invalid input prompt length {prompt.length()}"<li>"Cant find pid memory index {pid}."<li>"Cant find share memory in encode"<li>"Encode cast buffer header failed."<li>"Tokenizer encode wait sub process timeout."<li>"Invalid output token length " + std::to_string(tokenIdSize)"Encode cast buffer to int64 failed."<li>"Encode memset_s failed."<li>"[InferTokenizer::DownloadUrl] download fail: {e.what()}."<li>"[InferTokenizer::DownloadUrl] get unknown error."<li>"Messages token length must be in(0, {MAX_TOKENS_NUM)], but got {reqTokens.size()}"<li>"req should contain 'inputs' and the type should be array."<li>"Inputs count must be 1."<li>"Cannot find name in inputs or its type is not string."<li>"The length of name set in inputs exceeds 256."<li>"Cannot find shape in inputs or its type is not array"<li>"Shape element must be integer type."<li>"Shape must be at most two-dimensional, and the last element's value must be in (0, {MAX_TOKENS_NUM}]."<li>"Can not find datatype in inputs or its type is not string."<li>"Unsupported datatype, got {typeName}."<li>"Cannot find data in inputs or its type is not array."<li>"Data element must be no more than max token id length="MAX_TOKENS_NUM"."<li>"Data size should be {inputShape[0][1]}, got {body["data"].size()}."<li>"Data element must be integer type."<li>"The id can contain only digits, letters, underscores(_), hyphens(-) and no more than {MAX_INPUT_ID_LENGTH} words in length."<li>"Req should contain 'outputs' and the type should be array."<li>"Outputs size should be {inputDataType.size()}, got {outputs.size()}."<li>"Outputs name must not be null."<li>"Input conflict."<li>"Inputs must not be null."<li>"The type of inputs is abnormal."<li>"Inputs must be necessary and data type must be string and length in (0, {MAX_INPUTS_NUM}], but the length of inputs is {utf16.length()}."<li>"Req should contain 'inputs' and the type should be string."<li>"The text_input not found."<li>"text_input arr is empty."<li>"The type of text_input is abnormal for id = {logId}."<li>"text_input must be necessary and data type must be string and length in (0, {MAX_INPUTS_NUM}], but the length of inputs is {utf16.length()}"<li>"text_input token length must be in (0, {MAX_TOKENS_NUM}], but got {reqTokens.size()}."<li>"Contains not prompt or prompt null."<li>"Prompt arr is empty."<li>"The type of prompt is abnormal."<li>"Prompt must be necessary and data type must be string and length in (0, {MAX_INPUTS_NUM}], but the length of inputs is {utf16.length()}."<li>"Param stream not boolean."<li>"repetition_penalty must be in (0.0, 2.0], got {jsonObj[key]}."<li>"Input validation error: `stop` item count more than {MAX_STOP_STRING_NUM}, but got {stopStrings.size()}"<li>"Input validation error: `stop` must be list[string] if list, and item length in [1,  {MAX_STOP_STRING_LEN}] with total <= {MAX_TOTAL_STOP}."<li>"Input validation error: length of `stop` must be in [1, {MAX_STOP_STRING_LEN} but got {GetU16Str(stopStrings).length()}."<li>"Param stop must be string or list[string]."<li>"presence_penalty not float number."<li>"presence_penalty not in [-2.0, 2.0]."<li>"frequency_penalty not number."<li>"frequency_penalty not in [-2.0, 2.0]."<li>"Request param include_stop_str_in_output is not support in dmi mode."<li>"max_tokens not number_integer."<li>"max_tokens must be (0, 2147483647], got {jsonObj[key]}."<li>"temperature must be in [0.0,2.0], got {inputNum}."<li>"top_k must be in [0,2147483647], got {jsonObj[key]}."<li>"stop_token_ids not array."<li>"stop_token_ids not list[int]."<li>"top must be float."<li>"top_p must be in (0.0, 1.0], got {jsonObj[key]}."<li>"Check open ai req parameter error"<li>"typical_p must be in (0.0,1.0], got {jsonObj[key]}."<li>"Check self develop req parameter error."<li>"truncate must be in (0,2147483647], got {body[key]}."<li>"The decoder_input_details must not be true when stream is true."<li>"The format of adapter_id is invalid."<li>"Check tgi req parameter error."<li>"firstTokenCost exceeds the max value of size_t type."<li>"The decodeTime length is invalid."<li>"The decodeTime exceeds the max value of size_t type."<li>"Check triton text req parameter error"<li>"repetition_penalty must be strictly positive, got {jsonObj[key]}."<li>"seed must be in (0, 18446744073709551615], got {jsonObj[key]}."<li>"temperature must be in (0.0, {MAX_FLOAT_VALUE}], got {jsonObj[key]}."<li>"top_k must be in [0,2147483647], got {jsonObj[key]}."<li>"top_p must be in (0.0,1.0], got {jsonObj[key]}."<li>"batch_size must be in (0,{MAX_INT32_VALUE}], got {jsonObj[key]}."<li>"max_new_tokens must be in (0, 2147483647], got {jsonObj[key]}."<li>"priority must be in [1,5], got {jsonObj[key]}."<li>"timeout must be strictly positive and small than 1 hour, got {jsonObj[key]}."<li>"Check triton token req parameter error."<li>"seed is not number unsigned."<li>"Input validation error: param stop must be string or list[string]."<li>"model must be string type."<li>"repetition_penalty must be in (0.0,2.0], got {jsonObj[key]}."<li>"Check vllm req parameter error."<li>"Failed to parse context to json body."<li>"Not found {key}."<li>"key must not be null."<li>"key must be {type} type."<li>"The id no more than {MAX_INPUT_ID_LENGTH} words in length."<li>"Stop request id = {stopReqId} is invalid."<li>"Req body converts to json fail. Reset to previous node status."<li>"Error: `inputs` or `prompt` must be necessary and data type must be string. Additionally, the request body must be valid json."<li>"reqType does not match type InferReqType."<li>"stream must be boolean type."<li>"Req body converts to json fail."<li>"Inputs in request body is invalid."<li>"Inputs must be necessary and data type must be string and length in [0, {MAX_INPUTS_NUM}], but the length of inputs is {utf16.length()}."<li>"TikToken process fail."<li>"Failed to check the input text. Can't convert string to UTF-16."|
|424|Generation Error|{<br>"type": "Generation Error",<br>"error": error message，详见错误信息列表<br>}|<li>"Failed to generate infer request."<li>"Failed to enqueue inferRequest: remainBlocks is nullptr."<li>"Failed to enqueue inferRequest: remainPrefillSlots is nullptr."<li>"Failed to enqueue inferRequest: remainPrefill is nullptr."|
|500|Incomplete Generation|{<br>"type": "Incomplete Generation",<br>"error": error message，详见错误信息列表<br>}|<li>"Failed to generate simulate infer request."<li>"Stop request failed, internal failed."<li>"Failed to enqueue inferRequest: backend manager has not been initialized."<li>"Failed to enqueue inferRequest: engine has not been initialized."<li>"Failed to enqueue inferRequest: Model instance has been finalized or not initialized."<li>"Failed to enqueue inferRequest: llmInferEngine is not initialized!"<li>"Failed forward in for infer engine."<li>"Engine callback timeout."<li>"Failed to get engine response."<li>"Health status changed during health detector."<li>"[P Node] Failed to get tokenIds from inferResponse."<li>"[P Node] Failed to decode tokenIds to respStr."<li>"[P Node] Send D request failed!"<li>"[P Node] Unknown transfer flag."<li>"Role status is invalid."<li>"Environment variable MIES_SERVICE_MONITOR_MODE is not set."<li>"Failed to get health status."<li>"No contact node detected."|
|503|Service Unavailable|{<br>"type": "Service Unavailable",<br>"error": error message，详见错误信息列表<br>}|<li>"The service has been stopped."<li>"Parse req json failed. Reset to previous node status."<li>"The server cannot process the inference request due to an unknown status."|


